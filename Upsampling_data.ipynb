{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "!python3 -m spacy download en\n",
    "## Read file\n",
    "file_name = 'Clusters_all_data/GL_clusters_final_cat.csv'\n",
    "## Read file using pandas\n",
    "df = pd.read_csv(file_name)\n",
    "df=df[[\"Cleaned\",\"Final Category\"]]\n",
    "df.columns=[\"Message\",\"Intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load nltk's English stopwords as variable called 'stop' and don't find synonym of those words.\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "## Tokenizing sentence into token for finding synonym.\n",
    "def make_tokenizer(texts):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(texts)\n",
    "    return t\n",
    "\n",
    "tokenizer = make_tokenizer(df['Message'])    ## Message is column name\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['Message'])\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary of word index\n",
    "index_word = {}\n",
    "for word in tokenizer.word_index.keys():\n",
    "    index_word[tokenizer.word_index[word]] = word\n",
    "    \n",
    "## word list\n",
    "words = [value for key, value in index_word.items()]\n",
    "\n",
    "## Function to find synonym of words \n",
    "import spacy\n",
    "nlp = spacy.load('en', parser=False)\n",
    "def check_lemma(t,w) :\n",
    "    r = [d for d in t if (nlp(d.text)[0].lemma_ != nlp(w.text)[0].lemma_)]\n",
    "    return r\n",
    "\n",
    "def get_word_synonym(word):\n",
    "  filtered_words = [w for w in word.vocab if (not w.lower_ in stop) and w.is_lower == word.is_lower and w.prob >= -15] ## (not w.lower_ in stop) and\n",
    "  similarity = sorted(filtered_words, key=lambda w: word.similarity(w), reverse=True)\n",
    "  filtered_similarity = check_lemma(similarity[:30], word)\n",
    "  return filtered_similarity[:3]\n",
    "\n",
    "## Synonym dictionary\n",
    "synonym_dict = {}\n",
    "\n",
    "for word in words:\n",
    "        synonym_dict.update({word : tuple([w.lower_ for w in get_word_synonym(nlp.vocab[word])])})\n",
    "        #print(word, \" : \", [w.lower_ for w in get_word_synonym(nlp.vocab[word])])\n",
    "        \n",
    "## Only consider filtered synonym\n",
    "import collections\n",
    "value_occurrences = collections.Counter(synonym_dict.values())\n",
    "\n",
    "filtered_synonym = {key: value for key, value in synonym_dict.items() if value_occurrences[value] == 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for augmenting data by replacing words with synonym using spaCy\n",
    "## This might not be best best method compared to data augmentation using language translation\n",
    "import re\n",
    "import random\n",
    "sr = random.SystemRandom()\n",
    "split_pattern = re.compile(r'\\s+')\n",
    "def data_augmentation(message, aug_range=1) :\n",
    "    augmented_messages = []\n",
    "    for j in range(0,aug_range) :\n",
    "        new_message = \"\"\n",
    "        for i in filter(None, split_pattern.split(message)) :\n",
    "            new_message = new_message + \" \" + sr.choice(filtered_synonym.get(i,[i]))\n",
    "        augmented_messages.append(new_message)\n",
    "    return augmented_messages\n",
    "\n",
    "## Dictionary for intent count\n",
    "## Intent is column name\n",
    "intent_count = df.Intent.value_counts().to_dict()\n",
    "\n",
    "## Get max intent count to match other minority classes through data augmentation\n",
    "import operator\n",
    "max_intent_count = max(intent_count.items(), key=operator.itemgetter(1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 888/888 [00:02<00:00, 420.02it/s]\n",
      "100%|██████████| 834/834 [00:02<00:00, 410.52it/s]\n",
      "100%|██████████| 772/772 [00:01<00:00, 438.11it/s]\n",
      "100%|██████████| 714/714 [00:01<00:00, 433.44it/s]\n",
      "100%|██████████| 484/484 [00:01<00:00, 395.52it/s]\n",
      "100%|██████████| 403/403 [00:01<00:00, 394.31it/s]\n",
      "100%|██████████| 169/169 [00:00<00:00, 407.37it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 330.77it/s]\n",
      "100%|██████████| 91/91 [00:00<00:00, 224.45it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 199.84it/s]\n"
     ]
    }
   ],
   "source": [
    "## Loop to interate all messages\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm\n",
    "newdf = pd.DataFrame()\n",
    "for intent, count in intent_count.items() :\n",
    "    count_diff = max_intent_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in tqdm.tqdm(df[df[\"Intent\"] == intent][\"Message\"]) :\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame([message], columns=['Message'])\n",
    "            dummy1[\"Intent\"] = intent\n",
    "            old_message_df = old_message_df.append(dummy1)\n",
    "            \n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = data_augmentation(message,  multiplication_count)\n",
    "            dummy2 = pd.DataFrame(new_messages, columns=['Message'])\n",
    "            dummy2[\"Intent\"] = intent\n",
    "            new_message_df = new_message_df.append(dummy2)\n",
    "        \n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points\n",
    "        newdf = newdf.append([old_message_df,new_message_df])\n",
    "    else :\n",
    "        newdf = newdf.append(df[df[\"Intent\"] == intent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4569</th>\n",
       "      <td>claim for damage to gas station pump insured w...</td>\n",
       "      <td>Accidental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4570</th>\n",
       "      <td>claim for damage to parking deck insured struc...</td>\n",
       "      <td>Accidental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>claim for damage to commercial property stage ...</td>\n",
       "      <td>Accidental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4572</th>\n",
       "      <td>claim for damage to motel 6 insured struck ove...</td>\n",
       "      <td>Accidental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>commercial property claim for marketing materi...</td>\n",
       "      <td>Accidental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claim a f fire in customer's home.</td>\n",
       "      <td>Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claim arising from fire that broke out at ins...</td>\n",
       "      <td>Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auto liability claim for damage to fire hydra...</td>\n",
       "      <td>Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>claim alleging fire in unit. allegations on o...</td>\n",
       "      <td>Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insured was working on a fire suppression sys...</td>\n",
       "      <td>Fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14278 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Message      Intent\n",
       "4569  claim for damage to gas station pump insured w...  Accidental\n",
       "4570  claim for damage to parking deck insured struc...  Accidental\n",
       "4571  claim for damage to commercial property stage ...  Accidental\n",
       "4572  claim for damage to motel 6 insured struck ove...  Accidental\n",
       "4573  commercial property claim for marketing materi...  Accidental\n",
       "...                                                 ...         ...\n",
       "1                    claim a f fire in customer's home.        Fire\n",
       "0      claim arising from fire that broke out at ins...        Fire\n",
       "0      auto liability claim for damage to fire hydra...        Fire\n",
       "14     claim alleging fire in unit. allegations on o...        Fire\n",
       "0      insured was working on a fire suppression sys...        Fire\n",
       "\n",
       "[14278 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print count of all new data points\n",
    "newdf.to_csv(\"GL Datasets/Upsampled_GL_AutoML.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
