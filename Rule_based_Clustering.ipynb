{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from language_detector import detect_language\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import datetime\n",
    "import time\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import ast\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocessing and Cleaning data #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"Loss_Narrative_data.csv\")\n",
    "meta.shape\n",
    "print(meta.shape[0], \" Total rows available\")\n",
    "count = 0\n",
    "index = []\n",
    "for i in range(len(meta)):\n",
    "    #print(i)\n",
    "    if type(meta.iloc[i, 2])== float or meta.iloc[i, 2]==0:\n",
    "        count += 1\n",
    "    else:\n",
    "        index.append(i)\n",
    "len(index)\n",
    "print(len(index), \"Total rows available\")\n",
    "dataset=meta.iloc[index,:]\n",
    "dataset=dataset.reset_index()\n",
    "dataset.drop(\"index\", inplace = True, axis = 1)\n",
    "\n",
    "dataset\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "\n",
    "###################################\n",
    "#### sentence level preprocess ####\n",
    "###################################\n",
    "\n",
    "# lowercase + base filter\n",
    "# some basic normalization\n",
    "def f_base(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: processed string: see comments in the source code for more info\n",
    "    \"\"\"\n",
    "    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n",
    "    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n",
    "    # normalization 2: lower case\n",
    "    s = s.lower()\n",
    "    # normalization 3: \"&gt\", \"&lt\"\n",
    "    s = re.sub(r'&gt|&lt:', ' ', s)\n",
    "    # normalization 4: letter repetition (if more than 2)\n",
    "    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # normalization 5: non-word repetition (if more than 1)\n",
    "    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # normalization 6: string * as delimiter\n",
    "    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n",
    "    # normalization 7: stuff in parenthesis, assumed to be less informal\n",
    "    s = re.sub(r'\\(.*?\\)', ' ', s)\n",
    "    # normalization 8: xxx[?!]. -- > xxx.\n",
    "    s = re.sub(r'\\W+?\\.', '.', s)\n",
    "    # normalization 9: [.?!] --> [.?!] xxx\n",
    "    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n",
    "    # normalization 10: ' ing ', noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # normalization 11: noise text\n",
    "    s = re.sub(r'product received for free[.| ]', ' ', s)\n",
    "    # normalization 12: phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "#language detection\n",
    "# def f_lan(s):\n",
    "#     \"\"\"\n",
    "#     :param s: string to be processed\n",
    "#     :return: boolean (s is English)\n",
    "#     \"\"\"\n",
    "\n",
    "#     # some reviews are actually english but biased toward french\n",
    "#     return detect_language(s) in {'English'}\n",
    "\n",
    "\n",
    "###############################\n",
    "#### word level preprocess ####\n",
    "###############################\n",
    "\n",
    "# filtering out punctuations and numbers\n",
    "def f_punct(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with punct and number filter out\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "\n",
    "# selecting nouns, verb and adjective \n",
    "# def f_noun(w_list):\n",
    "#     \"\"\"\n",
    "#     :param w_list: word list to be processed\n",
    "#     :return: w_list with only nouns selected\n",
    "#     \"\"\"\n",
    "#     return [word for (word, pos) in nltk.pos_tag(w_list) \n",
    "#             if pos[:2] == 'NN' or pos=='JJ' or pos==\"VBD\"]\n",
    "\n",
    "\n",
    "# typo correction\n",
    "def f_typo(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "        if suggestions:\n",
    "            w_list_fixed.append(suggestions[0].term)\n",
    "        else:\n",
    "            w_list_fixed.append(word)\n",
    "            # do word segmentation, deprecated for inefficiency\n",
    "            # w_seg = sym_spell.word_segmentation(phrase=word)\n",
    "            # w_list_fixed.extend(w_seg.corrected_string.split())\n",
    "    return w_list_fixed\n",
    "\n",
    "\n",
    "# stemming if doing word-wise\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def f_stem(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with stemming\n",
    "    \"\"\"\n",
    "    \n",
    "    return [lemmatizer.lemmatize(word ,'v') for word in w_list]\n",
    "\n",
    "    \n",
    "# filtering out stop words\n",
    "# create English stop words list\n",
    "import calendar\n",
    "import datetime\n",
    "# Month name from number\n",
    "month_num = 1\n",
    "month_abre = datetime.date(2015, month_num, 1).strftime('%b')\n",
    "month_name = datetime.date(2015, month_num, 1).strftime('%B')\n",
    "\n",
    "# Print list of all months from calendar\n",
    "month_abr=[]\n",
    "month=[]\n",
    "for month_val in range(1, 13):\n",
    "    month_abr.append(calendar.month_abbr[month_val])\n",
    "    month.append(calendar.month_name[month_val])\n",
    "month_abrev=[word.lower() for word in month_abr ] \n",
    "months=[word.lower() for word in month ]\n",
    "months.extend(month_abrev)\n",
    "\n",
    "stop_words = (list(\n",
    "    set(get_stop_words('en'))\n",
    "    |set(months)\n",
    "))\n",
    "# stop_words.extend([\"claim\",'id','dopa',\"arise\",\"claimant\",\"allegations\",\"auto\",\"ploicy\",\"care\",\"work\",\"potential\",\n",
    "#                    \"service\",\"allegdly\",\"allege\",\"clmt\",\"insd\",'insure','incident','covid-19','covid','complaint',\n",
    "#                   'ovid','state','district','plaintiff','instal','due','demand','locate',\n",
    "#                    'cause','client','customer','date','understand','app'])\n",
    "\n",
    "\n",
    "def f_stopw(w_list):\n",
    "    \"\"\"\n",
    "    filtering out stop words\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word not in stop_words]\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "      cleanr = re.compile('<.*?>')\n",
    "      cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "      return cleantext\n",
    "  \n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw review texts\n",
    "    :param rw: review to be processed\n",
    "    :return: sentence level pre-processed review\n",
    "    \"\"\"\n",
    "    s= f_base(rw)\n",
    "    s = re.sub(r\"\\bac\\b\",\"coolant\",s)\n",
    "    s = re.sub(r\"\\ba/c\\b\",\"coolant\",s)\n",
    "    s = re.sub(r\"\\btv\\b\",\"telivision\",s)\n",
    "    s = re.sub(r\"\\bair conditioning\\b\",\"coolant\",s)\n",
    "    s = re.sub(r\"\\bhard drive\\b\",\"disk\",s)\n",
    "    s = re.sub(r\"\\bpd\\b\",\"property damage\",s)\n",
    "    s = re.sub(r\"\\bbpp\\b\",\"business personal property\",s)\n",
    "    s = re.sub(r\"\\binj\\b\",\"injury\",s)\n",
    "    s = re.sub(r\"\\bbi\\b\",\"bodily injury\",s)\n",
    "    s = re.sub(r\"\\bdol\\b\",\"date of loss\",s)\n",
    "    s = re.sub(r\"\\bfcdpa\\b\",\"the fair debt collection practices act\",s)\n",
    "    s = re.sub(r\"\\bfdpca\\b\",\"the fair debt collection practices act\",s)\n",
    "    s = re.sub(r\"\\bfdcpa\\b\",\"the fair debt collection practices act\",s)\n",
    "    s = re.sub(r\"\\btcpa\\b\",\"telephone consumer protection act\",s)\n",
    "    s = re.sub(r\"\\bcpa\\b\",\"consumer protection act\",s)\n",
    "    s = re.sub(r\"\\bcccraa\\b\",\"consumer credit reporting agencies act\",s)\n",
    "    s = re.sub(r\"\\bhud\\b\",\"housing and urban development\",s)\n",
    "    s = re.sub(r\"\\bada\\b\",\"americans with disabilities act\",s)\n",
    "    s = re.sub(r\"\\bfcra\\b\",\"fair credit reporting act\",s)\n",
    "    s = re.sub(r\"\\bfcpa\\b\",\"The foreign corrupt practices act\",s)\n",
    "    s = re.sub(r\"\\bfha\\b\",\"fha fair housing act\",s)\n",
    "    s = re.sub(r\"\\bfacta\\b\",\"fair and accurate credit transaction Acts\",s)\n",
    "    s = re.sub(r\"\\ddtpa\\b\",\"deceptive trade practices act\",s)\n",
    "    s = re.sub(r\"\\beeoc\\b\",\"equal employment opportunity commission\",s)\n",
    "    s = re.sub(r\"\\bbrea\\b\",\"business research and economic advisors\",s)\n",
    "    s = re.sub(r\"\\bfdupta\\b\",\"florida deceptive and unfair trade practices act\",s) \n",
    "    s = re.sub(r\"\\bwsba\\b\",\"washington state bar association\",s)\n",
    "    s = re.sub(r\"\\bfccpa\\b\",\"florida consumer collection practices act\",s)\n",
    "    s = re.sub(r\"\\bcfo\\b\",\"chief financial officer\",s)\n",
    "    s = re.sub(r\"\\bbacs\\b\",\"bankers automated clearing system\",s)\n",
    "    s=cleanhtml(s)\n",
    "    s=s.replace('n/a',\" \")\n",
    "    s=s.replace('nbsp',\" \")\n",
    "    s=s.replace(\"dol\",\" \")\n",
    "    if len(s)>0:\n",
    "        if s[0]==\" \":\n",
    "            s=s[1:]\n",
    "    if len(s)>0 :       \n",
    "        if s[0].isdigit():\n",
    "            s=s[2:]\n",
    "    if len(s)>0:       \n",
    "        if s[0]==\" \":\n",
    "            s=s[1:]\n",
    "    if len(s)>0:\n",
    "          if s[0]==\".\":\n",
    "            s=s[1:]\n",
    "    s=s.replace(\"’\",\"'\")\n",
    "    s=re.sub(\"[^A-Za-z0-9 . ' $]\",\" \",s)\n",
    "    s=re.sub(\"  +\",\" \",s)\n",
    "    s=s.replace('..','.')   \n",
    "    s=s.replace(\" . \",\". \")\n",
    "    s=s.replace(\"clmt\",\"claimant\")\n",
    "    s=s.replace(\"clmnt\",\"claimant\")\n",
    "    s=s.replace(\"insd\",'insured')\n",
    "    s=s.replace('dmg',\"damage\")\n",
    "    s=s.replace('dmgs',\"damages\")\n",
    "#     if not f_lan(s):\n",
    "#         return None\n",
    "    return s\n",
    "\n",
    "\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences\n",
    "    including: remove punctuation, select noun, fix typo, stem, stop_words\n",
    "    :param s: sentence to be processed\n",
    "    :return: word level pre-processed review\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = f_punct(w_list)  \n",
    "    w_list = f_typo(w_list)\n",
    "    w_list= f_stem(w_list)\n",
    "    #w_list = list(set(f_noun(w_list)))\n",
    "    w_list = f_stopw(w_list)\n",
    "    return w_list\n",
    "\n",
    "\n",
    "def preprocess(docs):\n",
    "    \"\"\"\n",
    "    Preprocess the data\n",
    "    \"\"\"\n",
    "    print('Preprocessing raw texts ... ')\n",
    "    n_docs = len(docs)\n",
    "    sentences = []  # sentence level preprocessed\n",
    "    token_lists = []  # word level preprocessed\n",
    "    idx_in = [] # index of sample selected\n",
    "    ref_id=[]\n",
    "    groups=[]\n",
    "    real_sen=[]\n",
    "    #     samp = list(range(100))\n",
    "    np.random.seed(1337)\n",
    "    samp = list(range(len(docs)))\n",
    "    for i, idx in enumerate(samp):\n",
    "        sentence = preprocess_sent(docs[idx])\n",
    "        token_list = preprocess_word(sentence)\n",
    "        if token_list:\n",
    "            idx_in.append(idx)\n",
    "            sentences.append(sentence)\n",
    "            token_lists.append(token_list)\n",
    "            ref_id.append(data.iloc[i,0])\n",
    "            groups.append(data.iloc[i,1])\n",
    "            real_sen.append(cleanhtml(docs[idx]))\n",
    "        print('{}--' .format(i), end='\\r')\n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return real_sen,sentences, token_lists, ref_id, groups, idx_in\n",
    "data = dataset \n",
    "rws = data.Text\n",
    "real_sen,sentences, token_lists1, ref_id,groups,idx_in = preprocess(rws)\n",
    "data_combined=pd.DataFrame({'Id':ref_id,'Grouping':groups,\"Narration\":real_sen,'Text':sentences,'Cleaned':token_lists1})\n",
    "#data_combined.to_csv(\"Clusters_by_words/Cleaned_sentences_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First party category and Third party category ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_property=data_combined[(data_combined[\"Grouping\"]=='1st Party Property') | (data_combined[\"Grouping\"]==\"3rd Party Property\")]\n",
    "party_property.reset_index(inplace=True,drop=True)\n",
    "party_property.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "####### Fire & Damage words occuring together ######\n",
    "####################################################\n",
    "\n",
    "clean_data=party_property[\"Cleaned\"]\n",
    "token_data=[]\n",
    "for i in clean_data:\n",
    "    token_data.append(\" \".join( w for w in i))\n",
    "fire1=\"fire damage\"\n",
    "val=[]\n",
    "for idx,i in enumerate(token_data):\n",
    "    if fire1 in i:\n",
    "        val.append(idx)\n",
    "fire1_data = party_property.iloc[val, :]\n",
    "fire1_data[\"Label\"]=list([7]*len(fire1_data))\n",
    "new_data=party_property.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)     \n",
    "\n",
    "####################################################\n",
    "####### Water & Damage words occuring together #####\n",
    "####################################################\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_data=[]\n",
    "for i in clean_data:\n",
    "    token_data.append(\" \".join( w for w in i))\n",
    "water1=\"water damage\"\n",
    "val=[]\n",
    "for idx,i in enumerate(token_data):\n",
    "    if water1 in i:\n",
    "        val.append(idx)\n",
    "water1_data = new_data.iloc[val, :]\n",
    "water1_data[\"Label\"]=list([8]*len(water1_data))\n",
    "new_data=new_data.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)    \n",
    "\n",
    "#####################################################\n",
    "####### Floor & Damage words occuring together ######\n",
    "#####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_data=[]\n",
    "for i in clean_data:\n",
    "    token_data.append(\" \".join( w for w in i))\n",
    "floor1=\"floor damage\"\n",
    "val=[]\n",
    "for idx,i in enumerate(token_data):\n",
    "    if floor1 in i:\n",
    "        val.append(idx)\n",
    "floor1_data = new_data.iloc[val, :]\n",
    "floor1_data[\"Label\"]=list([10]*len(floor1_data))\n",
    "new_data=new_data.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)   \n",
    "\n",
    "####################################################\n",
    "##### Breah of Contract/Unsatissfactory work #######\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "breach=[\"breach\",\"contract\",\"agreement\",'conflict',\"unsatisfactorily\",\"dissatisfy\",\"incomplete\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in breach if x in i):\n",
    "          val1.append(idx)\n",
    "breach_data = new_data.iloc[val1, :]\n",
    "breach_data[\"Label\"]=list([1]*len(breach_data))\n",
    "new_data=new_data.drop(val1)\n",
    "\n",
    "####################################################\n",
    "######## Copyright Trademark infringement ##########\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "trademark=[\"copyright\",\"trademark\",\"infringement\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in trademark if x in i):\n",
    "          val1.append(idx)\n",
    "trade_data = new_data.iloc[val1, :]\n",
    "trade_data[\"Label\"]=list([2]*len(trade_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "################# Thef & Steal data ################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists) \n",
    "steal=[\"steal\",\"theft\",\"robbery\",\"burglary\",\"rob\"]\n",
    "val=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in steal if x in i):\n",
    "          val.append(idx)\n",
    "steal_data = new_data.iloc[val, :]\n",
    "steal_data[\"Label\"]=list([3]*len(steal_data))\n",
    "new_data=new_data.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############### Missing & Lost data ################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists) \n",
    "steal=[\"miss\",\"missing\",\"lost\"]\n",
    "val=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in steal if x in i):\n",
    "          val.append(idx)\n",
    "steal2_data = new_data.iloc[val, :]\n",
    "steal2_data[\"Label\"]=list([4]*len(steal2_data))\n",
    "new_data=new_data.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############# Equipment Damage data ################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "equip=[\"camera\",'computer',\"monitor\",\"appliances\",\"coolant\",\"fridge\",\"lamp\",\"duct\",\"stave\",\"compressor\",\"phones\",\n",
    "       \"equipment\",\"bag\",\"laptop\",\"photography\",\"bulb\",\"card\",\"screen\",\"phone\",'eyeglasses',\"microwave\",\n",
    "       ,\"drone\",\"backpack\",\"headset\",\"heaters\",\"lens\",\"ipad\",\"stove\",\"lamp\",\"shutter\",\"server\"\n",
    "       \"lenses\",\"tvs\",\"television\",\"tool\",\"refrigerator\",\"heater\",\"machine\",\"stool\",\"machine\",\"strap\",\"woofer\",\n",
    "       \"wire\",\"cable\",\"drop\",\"trimmer\",\"cameras\",\"mac\",\"laptops\",\"electronic\",\"pro\",\"board\",\"furnace\",\n",
    "       \"battery\",\"cornice\",\"flash\",\"printer\",\"light\",\"tripod\",\"keyboard\",\"tampon\",\"scanner\",\"valence\",\"battery\",\n",
    "       \"harddrive\",\"mirror\",\"nikon\",\"tablet\",\"photobooth\",\"projector\",\"display\",\"garner\",\"projectors\",\"fibre\",\n",
    "       \"speakers\",\"photos\",\"computers\",\"cellphone\",\"dryers\",\"tablet\",\"freezer\",\"disk\",\"blower\",\"band\",\"vacuum\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in equip if x in i):\n",
    "          val1.append(idx)\n",
    "equip_data = new_data.iloc[val1, :]\n",
    "equip_data[\"Label\"]=list([5]*len(equip_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############# Natural Disaster data ################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "natural=[\"hurricane\",\"storm\",\"windstorm\",\"tornado\",\"thunder\",\"wildfire\",\"wild\",\"hail\",\"rain\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in natural if x in i):\n",
    "          val1.append(idx)\n",
    "natural_data = new_data.iloc[val1, :]\n",
    "natural_data[\"Label\"]=list([6]*len(natural_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "################# Fire Damage Data #################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "fire=[\"fire\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in fire if x in i):\n",
    "          val1.append(idx)\n",
    "fire_data = new_data.iloc[val1, :]\n",
    "fire_data[\"Label\"]=list([7]*len(fire_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "################## Flood Damage Data ###############\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "water=[\"flood\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in water if x in i):\n",
    "          val1.append(idx)\n",
    "water_data2 = new_data.iloc[val1, :]\n",
    "water_data2[\"Label\"]=list([88]*len(water_data2))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "################ Mold Damage Data ##################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "water=[\"mold\",\"mould\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in water if x in i):\n",
    "          val1.append(idx)\n",
    "water_data3 = new_data.iloc[val1, :]\n",
    "water_data3[\"Label\"]=list([888]*len(water_data3))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############## Water Damage Data ###################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "water=[\"water\",\"leak\",'plumb',\"line\",\"puncture\",\n",
    "       \"rain\",\"supply\",\"flood\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in water if x in i):\n",
    "          val1.append(idx)\n",
    "water_data = new_data.iloc[val1, :]\n",
    "water_data[\"Label\"]=list([8]*len(water_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############## Vehicle Damage Data #################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "veh=[\"vehicle\",\"car\",\"trailer\",\"truck\",\"auto\",\"drive\",\"truck\",\"accident\",\"cars\",\"tractor\",\"van\",\"boat\",\"bicycle\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in veh if x in i):\n",
    "          val1.append(idx)\n",
    "veh_data = new_data.iloc[val1, :]\n",
    "veh_data[\"Label\"]=list([9]*len(veh_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True) \n",
    "\n",
    "####################################################\n",
    "########## Accidental Property damage Data #########\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "prop=[\"window\",\"glass\",\"home\",\"break\",\"house\",\"lawn\",\"clean\",\"tree\",\"damage\",\"staircase\",\"wheelchair\",\"trimmer\",\"sink\",\"sewage\",\n",
    "       \"windows\",\"paint\",\"floor\",\"roof\",\"ceiling\",\"wall\",\"mold\",\"mould\",'ceiling',\"gazebo\",\"driveway\",\"mower\",\"windshield\",\n",
    "       \"carpet\",\"gate\",\"pool\",\"tile\",\"fence\",\"door\",\"doors\",\"furniture\",\"bathroom\",\"motor\",\"kitchen\",\"tub\",\"attic\",\"pipe\",\"property\",\n",
    "       \"mirror\",\"bathtub\",'pipelines',\"granite\",\"construction\",\"concrete\",\"chair\",\"char\",\"furnace\",\"ladder\",\"dent\",\"chemical\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in prop if x in i):\n",
    "          val1.append(idx)\n",
    "prop_data = new_data.iloc[val1, :]\n",
    "prop_data[\"Label\"]=list([10]*len(prop_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############### Data/Cyber Lost ####################\n",
    "####################################################\n",
    "\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "lostdata=[\"data\",\"drive\",\"disk\",\"hack\",\"hacking\",\"website\",\"software\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in lostdata if x in i):\n",
    "          val1.append(idx)\n",
    "lost_data = new_data.iloc[val1, :]\n",
    "lost_data[\"Label\"]=list([11]*len(lost_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "################ Miscellaneous Data ################\n",
    "####################################################\n",
    "\n",
    "new_data['Label']=list([12]*len(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustered_first=pd.concat([steal_data,steal2_data,equip_data,natural_data,fire1_data,fire_data,water1_data,water_data,water_data2,water_data3,\n",
    "                           veh_data,prop_data,floor1_data,lost_data,breach_data,trade_data,new_data],axis=0)\n",
    "Clustered_first.sort_values(by=[\"Label\"],inplace=True)\n",
    "Clustered_first.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustered_first.to_csv(\"clustered property.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### General Liability Data #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "General_liability=data_combined[(data_combined[\"Grouping\"]=='GL')]\n",
    "General_liability.reset_index(inplace=True,drop=True)\n",
    "General_liability.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "####### Fire & Damage words occuring together ######\n",
    "####################################################\n",
    "\n",
    "clean_data=General_liability[\"Cleaned\"]\n",
    "token_data=[]\n",
    "for i in clean_data:\n",
    "    token_data.append(\" \".join( w for w in i))\n",
    "fire1=\"fire damage\"\n",
    "val=[]\n",
    "for idx,i in enumerate(token_data):\n",
    "    if fire1 in i:\n",
    "        val.append(idx)\n",
    "fire1_data = General_liability.iloc[val, :]\n",
    "fire1_data[\"Label\"]=list([7]*len(fire1_data))\n",
    "new_data=General_liability.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)  \n",
    "\n",
    "####################################################\n",
    "####### Water & Damage words occuring together #####\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_data=[]\n",
    "for i in clean_data:\n",
    "    token_data.append(\" \".join( w for w in i))\n",
    "water1=\"water damage\"\n",
    "val=[]\n",
    "for idx,i in enumerate(token_data):\n",
    "    if water1 in i:\n",
    "        val.append(idx)\n",
    "water1_data = new_data.iloc[val, :]\n",
    "water1_data[\"Label\"]=list([8]*len(water1_data))\n",
    "new_data=new_data.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)    \n",
    "\n",
    "####################################################\n",
    "####### Floor & Damage words occuring together #####\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_data=[]\n",
    "for i in clean_data:\n",
    "    token_data.append(\" \".join( w for w in i))\n",
    "floor1=\"floor damage\"\n",
    "val=[]\n",
    "for idx,i in enumerate(token_data):\n",
    "    if floor1 in i:\n",
    "        val.append(idx)\n",
    "floor1_data = new_data.iloc[val, :]\n",
    "floor1_data[\"Label\"]=list([10]*len(floor1_data))\n",
    "new_data=new_data.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)   \n",
    "\n",
    "####################################################\n",
    "##### Bodily & Injury words occuring together ######\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_data=[]\n",
    "for i in clean_data:\n",
    "    token_data.append(\" \".join( w for w in i))\n",
    "body=\"bodily injury\"\n",
    "val=[]\n",
    "for idx,i in enumerate(token_data):\n",
    "    if body in i:\n",
    "        val.append(idx)\n",
    "body_data = new_data.iloc[val, :]\n",
    "body_data[\"Label\"]=list([4]*len(body_data))\n",
    "new_data=new_data.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)   \n",
    "\n",
    "####################################################\n",
    "###### Property & Damage words occuring together ###\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_data=[]\n",
    "for i in clean_data:\n",
    "    token_data.append(\" \".join( w for w in i))\n",
    "prop1=\"property damage\"\n",
    "val=[]\n",
    "for idx,i in enumerate(token_data):\n",
    "    if prop1 in i:\n",
    "        val.append(idx)\n",
    "prop1_data = new_data.iloc[val, :]\n",
    "prop1_data[\"Label\"]=list([10]*len(prop1_data))\n",
    "new_data=new_data.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)   \n",
    "\n",
    "\n",
    "####################################################\n",
    "###### Breach of Contract/Unsatisfactory Work ######\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "breach=[\"breach\",\"contract\",\"agreement\",'conflict',\"unsatisfactorily\",\"dissatisfy\",\"incomplete\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in breach if x in i):\n",
    "          val1.append(idx)\n",
    "breach_data = new_data.iloc[val1, :]\n",
    "breach_data[\"Label\"]=list([1]*len(breach_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "######## Copyright Trademark infringement ##########\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "trademark=[\"copyright\",\"trademark\",\"infringement\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in trademark if x in i):\n",
    "          val1.append(idx)\n",
    "trade_data = new_data.iloc[val1, :]\n",
    "trade_data[\"Label\"]=list([2]*len(trade_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############## Theft and Steal Data ################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists) \n",
    "steal=[\"steal\",\"theft\",\"robbery\",\"burglary\",\"miss\",\"missing\",\"rob\"]\n",
    "val=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in steal if x in i):\n",
    "          val.append(idx)\n",
    "steal_data = new_data.iloc[val, :]\n",
    "steal_data[\"Label\"]=list([3]*len(steal_data))\n",
    "new_data=new_data.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "####################################################\n",
    "############# Bodily Injury Data ###################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "injury=[\"injury\",\"injure\",\"slip\",\"trip\",'thigh',\"stairs\",\"ankle\",\"knee\",\"infection\",\"knee\",\n",
    "        \"ankle\",\"foot\",\"facial\",\"burns\",\"burn\",\"hurt\",\"fracture\",\"finger\",\"fingers\",\"patient\",\n",
    "        \"allergic\",\"eyebrow\",\"skin\",\"digest\",\"bodily\",\"skate\",\"hair\",\"dead\",\n",
    "        \"hand\",\"face\",\"injuries\",\"arm\",\"wrist\",'thighs','ear',\"bodily\",\"tooth\",\"bi\"\n",
    "        \"foot\",\"death\",\"dog\",\"bite\",\"leg\",\"stair\",\"head\",\"barbeque\",\"surgery\",\"throat\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in injury if x in i):\n",
    "          val1.append(idx)\n",
    "injury_data = new_data.iloc[val1, :]\n",
    "injury_data[\"Label\"]=list([4]*len(injury_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "################ Natural Disaster Data #############\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "natural=[\"flood\",\"hurricane\",\"storm\",\"windstorm\",\"tornado\",\"thunder\",\"wildfire\",\"wind\",\"hail\",\"rain\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in natural if x in i):\n",
    "          val1.append(idx)\n",
    "natural_data = new_data.iloc[val1, :]\n",
    "natural_data[\"Label\"]=list([5]*len(natural_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "####################################################\n",
    "################ Equipment Damage Data #############\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "equip=[\"camera\",'computer',\"monitor\",\"appliances\",\"coolant\",\"fridge\",\"lamp\",\"duct\",\"stave\",\"compressor\",\"phones\",\n",
    "       \"equipment\",\"bag\",\"laptop\",\"photography\",\"bulb\",\"card\",\"screen\",\"phone\",'eyeglasses',\"microwave\",\n",
    "       \"table\",\"drone\",\"backpack\",\"headset\",\"heaters\",\"lens\",\"ipad\",\"stove\",\"lamp\",\"shutter\",\"server\"\n",
    "       \"lenses\",\"tvs\",\"television\",\"tool\",\"refrigerator\",\"heater\",\"machine\",\"stool\",\"strap\",\"woofer\",\n",
    "       \"wire\",\"cable\",\"drop\",\"trimmer\",\"cameras\",\"mac\",\"laptops\",\"pro\",\"board\",\"furnace\",\"telivisions\"\n",
    "      \"battery\",\"cornice\",\"flash\",\"printer\",\"light\",\"tripod\",\"keyboard\",\"tampon\",\"scanner\",\"valence\",\"battery\",\n",
    "       \"harddrive\",\"mirror\",\"nikon\",\"tablet\",\"photobooth\",\"projector\",\"display\",\"garner\",\"projectors\",\"fibre\",\n",
    "      \"speakers\",\"photos\",\"computers\",\"cellphone\",\"dryers\",\"tablet\",\"freezer\",\"disk\",\"blower\",\"band\",\"vacuum\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in equip if x in i):\n",
    "          val1.append(idx)\n",
    "equip_data = new_data.iloc[val1, :]\n",
    "equip_data[\"Label\"]=list([6]*len(equip_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "################### Fire Damage Data ###############\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "fire=[\"fire\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in fire if x in i):\n",
    "          val1.append(idx)\n",
    "fire_data = new_data.iloc[val1, :]\n",
    "fire_data[\"Label\"]=list([7]*len(fire_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "################ Water Damage Data #################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "water=[\"water\",\"leak\",'plumb',\"line\",\"rain\",\"supply\",\"puncture\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in water if x in i):\n",
    "          val1.append(idx)\n",
    "water_data = new_data.iloc[val1, :]\n",
    "water_data[\"Label\"]=list([8]*len(water_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "################ Vehicle Damage Data ###############\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "veh=[\"vehicle\",\"car\",\"trailer\",\"truck\",\"auto\",\"drive\",\"truck\",\"accident\",\"cars\",\"tractor\",\"van\",\"boat\",\"bicycle\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in veh if x in i):\n",
    "          val1.append(idx)\n",
    "veh_data = new_data.iloc[val1, :]\n",
    "veh_data[\"Label\"]=list([9]*len(veh_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "######### Accidental Property Damage Data ##########\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "prop=[\"window\",\"glass\",\"home\",\"break\",\"house\",\"lawn\",\"clean\",\"tree\",\"damage\",\"staircase\",\"wheelchair\",\"trimmer\",\"sink\",\"sewage\",\n",
    "      \"windows\",\"paint\",\"floor\",\"roof\",\"ceiling\",\"wall\",\"mold\",\"mould\",'ceiling',\"gazebo\",\"driveway\",\"mower\",\"windshield\"\n",
    "      \"carpet\",\"gate\",\"pool\",\"tile\",\"fence\",\"door\",\"doors\",\"furniture\",\"bathroom\",\"motor\",\"kitchen\",\"tub\",\"attic\",\"pipe\",\n",
    "      \"mirror\",\"bathtub\",'pipelines',\"granite\",\"construction\",\"concrete\",\"chair\",\"char\",\"furnace\",\"ladder\",\"dent\",\"chemical\",\n",
    "      \"sofa\",\"couch\",\"windshield\",\"pot\",\"stain\",\"property\",\"carpet\",\"spill\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in prop if x in i):\n",
    "          val1.append(idx)\n",
    "prop_data = new_data.iloc[val1, :]\n",
    "prop_data[\"Label\"]=list([10]*len(prop_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############### Data/Cyber Lost ####################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "lostdata=[\"data\",\"drive\",\"disk\",\"hack\",\"hacking\",\"website\",\"software\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in lostdata if x in i):\n",
    "          val1.append(idx)\n",
    "lost_data = new_data.iloc[val1, :]\n",
    "lost_data[\"Label\"]=list([11]*len(lost_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "########### Fall/Slip Damage Data(BI) ###############\n",
    "####################################################\n",
    "\n",
    "injury_data.reset_index(inplace=True,drop=True)\n",
    "clean_data=injury_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "fall=[\"fall\",\"slip\",\"trip\",'fell']\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in fall if x in i):\n",
    "          val1.append(idx)\n",
    "fall_data1 = injury_data.iloc[val1, :]\n",
    "fall_data1[\"Label\"]=list([12]*len(fall_data1))\n",
    "injury_data=injury_data.drop(val1)\n",
    "injury_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "################### Fall Data ######################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "fall=[\"fall\",\"step\",\"steps\",\"trip\",'fell',\"slip\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in fall if x in i):\n",
    "          val1.append(idx)\n",
    "fall_data = new_data.iloc[val1, :]\n",
    "fall_data[\"Label\"]=list([12]*len(fall_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "################ Unknown Data ######################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists) \n",
    "uk=[\"withdraw\", \"withdrew\",\"incomplete\"]\n",
    "val0=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in uk if x in i):\n",
    "          val0.append(idx)\n",
    "uk_data = new_data.iloc[val0, :]\n",
    "uk_data[\"Label\"]=list(([13]*len(uk_data)))\n",
    "new_data=new_data.drop(val0)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "########### Miscellaneous Data #####################\n",
    "####################################################\n",
    "\n",
    "new_data['Label']=list([14]*len(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GL_clustered=pd.concat([fire1_data,water1_data,floor1_data,body_data,prop1_data,breach_data,trade_data,steal_data,injury_data,natural_data,\n",
    "equip_data,fire_data,water_data,veh_data,prop_data,lost_data,fall_data1,fall_data,uk_data,new_data],axis=0)\n",
    "GL_clustered.sort_values(by=[\"Label\"],inplace=True)\n",
    "GL_clustered.reset_index(inplace=True,drop=True)\n",
    "GL_clustered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GL_clustered.to_csv(\"GL_Clustered.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Error And Omissions Data #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EO_data=data_combined[(data_combined[\"Grouping\"]=='E&O')]\n",
    "EO_data.reset_index(inplace=True,drop=True)\n",
    "EO_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "################## Unkown Data #####################\n",
    "####################################################\n",
    "new_data=EO_data\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "      token_lists.append(i)\n",
    "len(token_lists) \n",
    "wrng=[\"withdraw\",\"duplicate\",\"cancel\",\"consolidate\",\"urban\"]\n",
    "val=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in wrng if x in i):\n",
    "          val.append(idx)\n",
    "wrng_data = new_data.iloc[val, :]\n",
    "wrng_data[\"Label\"]=list([1]*len(wrng_data))\n",
    "new_data=new_data.drop(val)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "####################################################\n",
    "############## Discrimination/Abuse Data ###########\n",
    "####################################################\\\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "dis=[\"discrimination\",\"rape\",\"sexual\",\"constitutional\",\"urban\",\n",
    "     \"racial\",\"abuse\",\"ethics\",\"cha\",\"harassment\",\"equal\",\"disabilities\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in dis if x in i):\n",
    "          val1.append(idx)\n",
    "dis_data = new_data.iloc[val1, :]\n",
    "dis_data[\"Label\"]=list([2]*len(dis_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "\n",
    "####################################################\n",
    "####### Copyright Trademark Infringement ###########\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "trademark=[\"copyright\",\"trademark\",\"infringement\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in trademark if x in i):\n",
    "          val1.append(idx)\n",
    "trade_data = new_data.iloc[val1, :]\n",
    "trade_data[\"Label\"]=list([3]*len(trade_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "####################################################\n",
    "################# Negligence Data ##################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "neg=[\"negligence\",\"professional\",\"negligent\",\"service\",\"tax\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in neg if x in i):\n",
    "          val1.append(idx)\n",
    "neg_data = new_data.iloc[val1, :]\n",
    "neg_data[\"Label\"]=list([4]*len(neg_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "####### Misrepresentation/Fraud Data ###############\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "mis=[\"misrepresentation\",\"misrepresentations\",\"fraud\",\"fraudlent\",\"disclosures\",\"disclosure\",\"trade\",\n",
    "     \"disclose\",\"scam\",\"corruption\",\"fraudulent\",\"scammed\",\"malpractice\",\"corrupt\",\"error\",\"representation\",\n",
    "     \"steal\",\"theft\",\"robbery\",\"burglary\",\"rob\",\"lost\",\"lose\",\"discrepancy\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in mis if x in i):\n",
    "          val1.append(idx)\n",
    "mis_data = new_data.iloc[val1, :]\n",
    "mis_data[\"Label\"]=list([5]*len(mis_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "### Breach Of Contract/Unsatisfactory Work Data ####\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "breach=[\"breach\",\"contract\",\"agreement\",\"incomplete\",\"unsatisfactory\",\"design\",\"dissatisfy\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in breach if x in i):\n",
    "          val1.append(idx)\n",
    "breach_data = new_data.iloc[val1, :]\n",
    "breach_data[\"Label\"]=list([6]*len(breach_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "####################################################\n",
    "############# Lost/Cyber Data ######################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "lostdata=[\"data\",\"photographs\",\"photograph\",\"software\",\"hack\",\"hacking\",\"disk\",\"website\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in lostdata if x in i):\n",
    "          val1.append(idx)\n",
    "lost_data = new_data.iloc[val1, :]\n",
    "lost_data[\"Label\"]=list([7]*len(lost_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############# Financial Damage Data ################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "money=[\"misappropriation\",\"debts\",\"debt\",\"repossession\",\"credit\",\"banker\",\"bankers\",\"bank\",\"telephone\",\"foreclosure\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in money if x in i):\n",
    "          val1.append(idx)\n",
    "money_data = new_data.iloc[val1, :]\n",
    "money_data[\"Label\"]=list([8]*len(money_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############### Bodily Injury Data #################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "injury=[\"injury\",\"injure\",\"slip\",\"trip\",'thigh',\"ankle\",\"knee\",\"bodily\",\"burn\",\"disability\",\n",
    "        \"hand\",\"face\",\"injuries\",\"arm\",\"wrist\",'thighs','ear',\"bodily\",\"tooth\",\"bi\",\"infection\",\n",
    "        \"foot\",\"dog\",\"bite\",\"death\",\"leg\",\"stair\",\"head\",\"surgery\",\"throat\",\"calf\",\"facial\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in injury if x in i):\n",
    "          val1.append(idx)\n",
    "bi_data = new_data.iloc[val1, :]\n",
    "bi_data[\"Label\"]=list([9]*len(bi_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############# Equipment Damage Data ################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "token_lists[1]\n",
    "equip=[\"camera\",'computer',\"monitor\",\"appliances\",\"coolant\",\"fridge\",\"duct\",\"compressor\",\"phones\",\n",
    "       \"equipment\",\"bag\",\"laptop\",\"photography\",\"bulb\",\"card\",\"screen\",\"phone\",'eyeglasses',\"microwave\",\n",
    "       \"drone\",\"backpack\",\"headset\",\"heaters\",\"lens\",\"ipad\",\"stove\",\"lamp\",\"shutter\",\"server\",\"speakers\",\n",
    "       \"lenses\",\"tvs\",\"television\",\"tool\",\"refrigerator\",\"heater\",\"machine\",\"stool\",\"machine\",\"strap\",\"woofer\",\n",
    "       \"cable\",\"cameras\",\"mac\",\"laptops\",\"furnace\",\n",
    "       \"cornice\",\"flash\",\"printer\",\"microsoft\",\"tripod\",\"keyboard\",\"tampon\",\"scanner\",\"battery\",\n",
    "       \"harddrive\",\"mirror\",\"nikon\",\"tablet\",\"projector\",\"display\",\"garner\",\"projectors\",\n",
    "       \"speakers\",\"photos\",\"computers\",\"cellphone\",\"dryers\",\"tablet\",\"freezer\",\"blower\",\"vacuum\",\"chandelier\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in equip if x in i):\n",
    "          val1.append(idx)\n",
    "equip_data = new_data.iloc[val1, :]\n",
    "equip_data[\"Label\"]=list([10]*len(equip_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############## Property Damage Data ################\n",
    "####################################################\n",
    "\n",
    "clean_data=new_data[\"Cleaned\"]\n",
    "token_lists=[]\n",
    "for i in clean_data:\n",
    "     token_lists.append(i)\n",
    "len(token_lists)  \n",
    "prop=[\"window\",\"glass\",\"home\",\"break\",\"house\",\"lawn\",\"clean\",\"tree\",\"bed\",\"staircase\",\"wheelchair\",\"trimmer\",\"sink\",\"sewage\",\n",
    "       \"windows\",\"paint\",\"floor\",\"roof\",\"ceiling\",\"wall\",'ceiling',\"gazebo\",\"driveway\",\"mower\",\"design\",\"property\",\"property\",\n",
    "       \"carpet\",\"gate\",\"pool\",\"tile\",\"fence\",\"door\",\"doors\",\"furniture\",\"bathroom\",\"motor\",\"kitchen\",\"tub\",\"attic\",\"pipe\",\n",
    "       \"mirror\",\"bathtub\",'pipelines',\"granite\",\"construction\",\"concrete\",\"chair\",\"char\",\"furnace\",\"ladder\",\"dent\",\"chemical\",\n",
    "        \"rental\",\"landlord\",\"rent\",\"apartment\",\"mismanagement\",\"estate\",\"mold\",\"mould\",\"water\",\"fire\"]\n",
    "val1=[]\n",
    "for idx,i in enumerate(token_lists):\n",
    "      if any(True for x in prop if x in i):\n",
    "          val1.append(idx)\n",
    "prop_data = new_data.iloc[val1, :]\n",
    "prop_data[\"Label\"]=list([11]*len(prop_data))\n",
    "new_data=new_data.drop(val1)\n",
    "new_data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "####################################################\n",
    "############# Miscellaneous Data ###################\n",
    "####################################################\n",
    "\n",
    "new_data[\"Label\"]=list([12]*len(new_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EO_Clustered=pd.concat([wrng_data,dis_data,trade_data,neg_data,mis_data,\n",
    "                          breach_data,lost_data,money_data,bi_data,equip_data,prop_data,new_data],axis=0)\n",
    "EO_Clustered.sort_values(by=[\"Label\"],inplace=True)\n",
    "EO_Clustered.reset_index(inplace=True,drop=True)\n",
    "EO_Clustered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EO_Clustered.to_csv(\"EO_Clustered.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
